{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import gc\nimport numpy as np\nimport cv2, glob\nfrom random import shuffle, randint\nfrom tqdm import tqdm\nimport pydot\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Reshape, BatchNormalization\nfrom tensorflow.keras.layers import Activation, Conv2DTranspose, Conv2D, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam,SGD,RMSprop\nfrom keras.layers.convolutional import _Conv\nfrom keras.legacy import interfaces\nfrom keras.engine import InputSpec\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils.vis_utils import plot_model\n\nfrom keras import Input\nfrom keras.layers import UpSampling2D,Concatenate\nfrom IPython.display import SVG\nfrom keras import Model\nfrom keras.models import model_from_json\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nK.tensorflow_backend._get_available_gpus()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_files_anime = glob.glob('../input/anime-faces/data/data/*.png')\nshuffle(image_files_anime)\nimage_files_train = glob.glob('../input/animeface/faces/faces/*.jpg')\nimage_files_face = glob.glob('../input/appa-real-face-cropped/final_files/final_files/*.jpg')\nshuffle(image_files_face)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_i = []\nc=0\nfor file in tqdm(image_files_train):\n    image = cv2.imread(file)\n    image = cv2.resize(image,(64,64))\n    image = image / 127.5 -1\n    x_i.append(image)\n    c+=1\n    if c==23000:\n        break\ngc.collect()\ntest = np.array(x_i[20000:])\ntest.shape\nx_i = np.array(x_i[:20000])\nx_i.shape\n\nx_t = []\nfor file in tqdm(image_files_face):\n    image = cv2.imread(file)\n    image = cv2.resize(image,(64,64))\n    image = image / 127.5 -1\n    x_t.append(image)\nx_t = np.array(x_t)\nx_t.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 4, ncols = 4, figsize = (16, 16))\nplt.setp(axes.flat, xticks = [], yticks = [])\nfor i, ax in enumerate(axes.flat):\n    index = randint(0, 7500)\n    ax.imshow(x_t[index][:,:,::-1],cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 4, ncols = 4, figsize = (16, 16))\nplt.setp(axes.flat, xticks = [], yticks = [])\nfor i, ax in enumerate(axes.flat):\n    index = randint(0, 7500)\n    ax.imshow(x_i[index][:,:,::-1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CycleGAN():\n    def __init__(self, img_rows=64, img_cols=64, channels=3):\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.channels = channels\n        \n        self.img_shape = (img_rows, img_cols, channels)\n        \n        self.disc_patch = (4,4,1)\n        self.residual_blocks = 6\n        \n        \n        self.gf = 32\n        self.df = 64\n        \n        self.lambda_cycle = 10.0\n        \n        self.lambda_id = 0.9 * self.lambda_cycle\n        \n        optimizer = Adam(0.0002, 0.5)\n        \n        self.d_a = self.build_discriminator('Discriminator_A')\n        self.d_b = self.build_discriminator('Discriminator_B')\n\n        self.d_a.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n        self.d_b.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n\n        self.g_ab = self.build_generator('Generator_AB')\n        self.g_ba = self.build_generator('Generator_BA')\n        \n        img_a = Input(shape=self.img_shape,name='Input_A')\n        img_b = Input(shape=self.img_shape,name='Input_B')\n        \n        fake_b = self.g_ab(img_a)\n        fake_a = self.g_ba(img_b)\n        \n        recon_a = self.g_ba(fake_b)\n        recon_b = self.g_ab(fake_a)\n        \n        img_a_id = self.g_ba(img_a)\n        img_b_id = self.g_ab(img_b)\n        \n        self.d_a.trainable = False\n        self.d_b.trainable = False\n        \n        valid_a = self.d_a(fake_a)\n        valid_b = self.d_b(fake_b)\n        \n        self.combined = Model(inputs=[img_a, img_b], output=[valid_a, valid_b, recon_a, recon_b, img_a_id, img_b_id])\n        self.combined.compile(loss=['mse', 'mse', 'mse','mse','mae','mae'],\n                             loss_weights=[1, 1, self.lambda_cycle, self.lambda_cycle, self.lambda_id, self.lambda_id], optimizer=optimizer)\n        \n    def plot_graph(self,model=0):\n        if model == 'discriminator':\n            return plot_model(self.d_a, show_shapes = True, show_layer_names = True,to_file='d.png')\n        elif model == 'generator':\n            return plot_model(self.g_ab, show_shapes = True, show_layer_names = True,to_file='g.png')\n        else:\n            return plot_model(self.combined, show_shapes = True, show_layer_names = True,to_file='c.png')\n        \n    @staticmethod\n    def conv2d(layer_input, filters, f_size=4, normalization=True):\n        \"\"\"Layers used during downsampling\"\"\"\n        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n        d = LeakyReLU(alpha=0.2)(d)\n        if normalization:\n            d = BatchNormalization()(d)\n\n        return d\n\n    @staticmethod\n    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0.3):\n        \"\"\"Layers used during upsampling\"\"\"\n        u = UpSampling2D(size=2)(layer_input)\n        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n        if dropout_rate:\n            u = Dropout(dropout_rate)(u)\n        u = BatchNormalization()(u)\n        u = Concatenate()([u, skip_input])\n\n        return u\n    \n    @staticmethod\n    def residual_block(layer_input):\n        \"\"\"Residual block described in paper\"\"\"\n        d = Conv2D(64, kernel_size=3, strides=1, padding='same')(layer_input)\n        d = BatchNormalization(momentum=0.8)(d)\n        d = Activation('relu')(d)\n        d = Conv2D(64, kernel_size=3, strides=1, padding='same')(d)\n        d = BatchNormalization(momentum=0.8)(d)\n        d = Concatenate()([d, layer_input])\n        return d        \n\n    def build_generator(self,name):\n        \"\"\"U-net Generator\"\"\"\n        d0 = Input(shape=self.img_shape)\n        \n        d1 = self.conv2d(d0, self.gf)\n        d2 = self.conv2d(d1, self.gf * 2)\n        d3 = self.conv2d(d2, self.gf * 4)\n        d4 = self.conv2d(d3, self.gf * 8)\n        \n        u1 = self.deconv2d(d4, d3, self.gf * 4)\n        u2 = self.deconv2d(u1, d2, self.gf * 2)\n        u3 = self.deconv2d(u2, d1, self.gf)\n        \n        u4 = UpSampling2D(size=2)(u3)\n        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n        \n        return Model(d0, output_img,name=name)\n    def build_generator_resnet(self,name):\n        \"\"\"Resnet Generator\"\"\"\n\n        img = Input(shape=self.img_shape)\n\n        l1 = Conv2D(64, kernel_size=3, padding='same', activation='relu')(img)\n\n        r = self.residual_block(l1)\n        for _ in range(self.residual_blocks - 1):\n            r = self.residual_block(r)\n\n        output_img = Conv2D(self.channels, kernel_size=3, padding='same', activation='tanh')(r)\n\n        return Model(img, output_img,name=name)    \n        \n    def build_discriminator(self,name):\n        img = Input(shape=self.img_shape)\n        \n        d1 = self.conv2d(img, self.df, normalization=False)\n        d2 = self.conv2d(d1, self.df * 2)\n        d3 = self.conv2d(d2, self.df * 4)\n        d4 = self.conv2d(d3, self.df * 8)\n        \n        validity = Conv2D(1, kernel_size=4, strides=1, padding='same',activation='tanh')(d4)\n        \n        return Model(img, validity,name=name)\n    \n    def train(self, epochs, batch_size=1,save=False):\n        \n        g_loss_history = []\n        d_loss_history = []\n        \n        valid = np.ones((batch_size,) + self.disc_patch)\n        fake = np.zeros((batch_size,) + self.disc_patch)\n        if 1==1:\n            for epoch in tqdm(range(1, epochs + 1)):\n                \n                index = np.random.randint(0, x_t.shape[0], batch_size)\n                imgs_a = x_i[index]\n                imgs_b = x_t[index]\n                fake_b = self.g_ab.predict(imgs_a)\n                fake_a = self.g_ba.predict(imgs_b)\n                \n                da_loss_real = self.d_a.train_on_batch(imgs_a, valid)\n                da_loss_fake = self.d_a.train_on_batch(fake_a, fake)\n                da_loss = 0.5 * np.add(da_loss_real, da_loss_fake)\n\n                db_loss_real = self.d_b.train_on_batch(imgs_b, valid)\n                db_loss_fake = self.d_b.train_on_batch(fake_b, fake)\n                db_loss = 0.5 * np.add(db_loss_real, db_loss_fake)\n\n                d_loss = 0.5 * np.add(da_loss, db_loss)\n                \n                g_loss = self.combined.train_on_batch([imgs_a, imgs_b], [valid, valid, imgs_a, imgs_b, imgs_a, imgs_b])\n                \n                g_loss_history.append(g_loss)\n                d_loss_history.append(d_loss)\n                if epoch % 300 == 0:\n                    index = np.random.randint(0,876, batch_size)\n                    gen_AB=gan.g_ab.predict(x_i[index])\n                    gen_BA=gan.g_ba.predict(x_i[index])\n                    fig, axes = plt.subplots(nrows = 8, ncols = 4, figsize = (16, 16))\n                    plt.setp(axes.flat, xticks = [], yticks = [])\n                    for i, ax in enumerate(axes.flat):\n                        #ax.imshow(gen_BA[i])\n                        ax.imshow(gen_AB[i])\n                    plt.show()\n                    \n        plt.figure(figsize = (20, 8))\n        plt.plot(d_loss_history)\n        plt.title('Discriminator Loss History')\n        plt.show()\n        if save:\n            model_json = self.combined.to_json()\n            with open(\"gan.json\", \"w\") as json_file:\n                json_file.write(model_json)\n\n            #model.save_weights(\"model.h5\")\n            print(\"Model saved\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gan = CycleGAN()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gan.train(epochs=5000, batch_size=32,save=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dot = gan.plot_graph('generator')\ndot = gan.plot_graph('discriminator')\ndot = gan.plot_graph()\ndot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = np.random.randint(0,876, 16)\ngen_AB=gan.g_ab.predict(test[index])\ngen_BA=gan.g_ba.predict(test[index])\nfig, axes = plt.subplots(nrows = 4, ncols = 4, figsize = (16, 16))\nplt.setp(axes.flat, xticks = [], yticks = [])\nfor i, ax in enumerate(axes.flat):\n    #ax.imshow(gen_BA[i])\n    ax.imshow(cv2.cvtColor(gen_AB[i].astype('float32'), cv2.COLOR_BGR2RGB))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 4, ncols = 4, figsize = (16, 16))\nplt.setp(axes.flat, xticks = [], yticks = [])\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(cv2.cvtColor(test[index][i].astype('float32'), cv2.COLOR_BGR2RGB))\n    #ax.imshow(gen_BA[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n    def conv2d(layer_input, filters, f_size=4, normalization=True):\n        \"\"\"Layers used during downsampling\"\"\"\n        with tf.variable_scope(\"conv2\", reuse=tf.AUTO_REUSE):\n            def spectral_norm(w, iteration=1):\n                w_shape = w.shape.as_list()\n                w = tf.reshape(w, [-1, w_shape[-1]])\n\n                u = tf.get_variable(\"u\", [1, w_shape[-1]], initializer=tf.random_normal_initializer(), trainable=False)\n\n                u_hat = u\n                v_hat = None\n                for i in range(iteration):\n                    v_ = tf.matmul(u_hat, tf.transpose(w))\n                    v_hat = tf.nn.l2_normalize(v_)\n\n                    u_ = tf.matmul(v_hat, w)\n                    u_hat = tf.nn.l2_normalize(u_)\n\n                u_hat = tf.stop_gradient(u_hat)\n                v_hat = tf.stop_gradient(v_hat)\n\n                sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))\n\n                with tf.control_dependencies([u.assign(u_hat)]):\n                    w_norm = w / sigma\n                    w_norm = tf.reshape(w_norm, w_shape)\n                return w_norm\n            \n            \n            w = tf.get_variable(\"kernel\", shape=[4, 4, layer_input.get_shape()[-1], 3])\n            b = tf.get_variable(\"bias\", [3], initializer=tf.constant_initializer(0.0))\n        \n            x = tf.nn.bias_add(tf.nn.conv2d(input=layer_input, filter=spectral_norm(w), strides=[1, 2, 2, 1],padding='SAME'),b)\n            \n            d = LeakyReLU(alpha=0.2)(x)\n            \n            d = BatchNormalization()(d)\n\n            return d","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}